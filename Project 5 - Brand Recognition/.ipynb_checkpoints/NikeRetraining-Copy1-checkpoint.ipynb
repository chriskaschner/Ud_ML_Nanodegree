{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to the model weights files.\n",
    "weights_path = 'weights/vgg16_weights.h5'\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "train_data_dir = 'data2/train/attempt1'\n",
    "validation_data_dir = 'data2/validation/attempt1'\n",
    "nb_train_samples = 870\n",
    "nb_validation_samples = 260\n",
    "nb_epoch = 50\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # load the weights of the VGG16 networks\n",
    "    # (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "    # note: when there is a complete match between your model definition\n",
    "    # and your weight savefile, you can simply call model.load_weights(filename)\n",
    "    assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "    f = h5py.File(weights_path)\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    print('Model loaded.')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "            train_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples)\n",
    "    np.save(open('bottleneck_features_train.npy', 'w'), bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "            validation_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode=None,\n",
    "            shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples)\n",
    "    np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n",
    "    print \"model saved!\"\n",
    "\n",
    "\n",
    "def train_top_model():\n",
    "    print \"\\nlet's hit it\\n\"\n",
    "    train_data = np.load(open('bottleneck_features_train.npy'))\n",
    "    train_labels = np.array([0] * (nb_train_samples / 2) + [1] * (nb_train_samples / 2))\n",
    "\n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy'))\n",
    "    validation_labels = np.array([0] * (nb_validation_samples / 2) + [1] * (nb_validation_samples / 2))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu', W_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              nb_epoch=nb_epoch, batch_size=32,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Found 870 images belonging to 3 classes.\n",
      "Found 260 images belonging to 3 classes.\n",
      "model saved!\n"
     ]
    }
   ],
   "source": [
    "save_bottlebeck_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image_batch(image_paths, img_size=None, crop_size=None, color_mode=\"rgb\", out=None):\n",
    "    img_list = []\n",
    "    \n",
    "    for im_path in image_paths:\n",
    "        img = imread(im_path, mode='RGB')\n",
    "        if img_size:\n",
    "            img = imresize(img,img_size)\n",
    "            \n",
    "        img = img.astype('float32')\n",
    "        # We permute the colors to get them in the BGR order\n",
    "        if color_mode==\"bgr\":\n",
    "            img[:,:,[0,1,2]] = img[:,:,[2,1,0]]\n",
    "        # We normalize the colors with the empirical means on the training set\n",
    "        img[:, :, 0] -= 123.68 \n",
    "        img[:, :, 1] -= 116.779\n",
    "        img[:, :, 2] -= 103.939\n",
    "        img = img.transpose((2, 0, 1))\n",
    "\n",
    "        if crop_size:\n",
    "            img = img[:,(img_size[0]-crop_size[0])//2:(img_size[0]+crop_size[0])//2\n",
    "                      ,(img_size[1]-crop_size[1])//2:(img_size[1]+crop_size[1])//2]\n",
    "            \n",
    "        img_list.append(img)\n",
    "\n",
    "    img_batch = np.stack(img_list, axis=0)\n",
    "    if not out is None:\n",
    "        out.append(img_batch)\n",
    "    else:\n",
    "        return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the weights of the VGG16 networks\n",
    "# (trained on ImageNet, won the ILSVRC competition in 2014)\n",
    "# note: when there is a complete match between your model definition\n",
    "# and your weight savefile, you can simply call model.load_weights(filename)\n",
    "assert os.path.exists(weights_path), 'Model weights not found (see \"weights_path\" variable in script).'\n",
    "f = h5py.File(weights_path)\n",
    "for k in range(f.attrs['nb_layers']):\n",
    "    if k >= len(model.layers):\n",
    "        # we don't look at the last (fully-connected) layers in the savefile\n",
    "        break\n",
    "    g = f['layer_{}'.format(k)]\n",
    "    weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "    model.layers[k].set_weights(weights)\n",
    "f.close()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (32768, 256) and (8192, 256) are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6546273eb315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# classifier, including the top classifier,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# in order to successfully do fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# add the model on top of the convolutional base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m   2425\u001b[0m                                         ' elements.')\n\u001b[1;32m   2426\u001b[0m                     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m             \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m    721\u001b[0m     '''\n\u001b[1;32m    722\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.pyc\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     43\u001b[0m   result = _op_def_lib.apply_op(\"Assign\", ref=ref, value=value,\n\u001b[1;32m     44\u001b[0m                                 \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                                 use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     46\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    702\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    703\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    705\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2260\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2262\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2263\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2264\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1700\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1701\u001b[0m                          % op.type)\n\u001b[0;32m-> 1702\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.pyc\u001b[0m in \u001b[0;36m_AssignShape\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# but that is a sufficiently niche case that supporting it does\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# not seem worthwhile.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Chris/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    568\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         raise ValueError(\"Shapes %s and %s are not compatible\" %\n\u001b[0;32m--> 570\u001b[0;31m                          (self, other))\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (32768, 256) and (8192, 256) are not compatible"
     ]
    }
   ],
   "source": [
    "# Adding L2 Regularization\n",
    "from keras.regularizers import l2\n",
    "lmbda = 0.01\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu', W_regularizer=l2(lmbda)))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "model.add(top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 870 images belonging to 3 classes.\n",
      "Found 260 images belonging to 3 classes.\n",
      "Epoch 1/50\n",
      "870/870 [==============================] - 552s - loss: -2.7209 - acc: 0.4874 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 2/50\n",
      "870/870 [==============================] - 560s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5140 - val_acc: 0.7654\n",
      "Epoch 3/50\n",
      "870/870 [==============================] - 564s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.8819 - val_acc: 0.7654\n",
      "Epoch 4/50\n",
      "870/870 [==============================] - 564s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.7593 - val_acc: 0.7577\n",
      "Epoch 5/50\n",
      "870/870 [==============================] - 559s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.1461 - val_acc: 0.8115\n",
      "Epoch 6/50\n",
      "870/870 [==============================] - 550s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5753 - val_acc: 0.7769\n",
      "Epoch 7/50\n",
      "870/870 [==============================] - 550s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.1461 - val_acc: 0.7731\n",
      "Epoch 8/50\n",
      "870/870 [==============================] - 549s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2687 - val_acc: 0.7731\n",
      "Epoch 9/50\n",
      "870/870 [==============================] - 548s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.3914 - val_acc: 0.7731\n",
      "Epoch 10/50\n",
      "870/870 [==============================] - 547s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 11/50\n",
      "870/870 [==============================] - 547s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.6979 - val_acc: 0.7538\n",
      "Epoch 12/50\n",
      "870/870 [==============================] - 546s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.3300 - val_acc: 0.7769\n",
      "Epoch 13/50\n",
      "870/870 [==============================] - 545s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5140 - val_acc: 0.7731\n",
      "Epoch 14/50\n",
      "870/870 [==============================] - 546s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2687 - val_acc: 0.7885\n",
      "Epoch 15/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.0848 - val_acc: 0.7846\n",
      "Epoch 16/50\n",
      "870/870 [==============================] - 546s - loss: -3.4512 - acc: 0.5402 - val_loss: -1.9008 - val_acc: 0.7962\n",
      "Epoch 17/50\n",
      "870/870 [==============================] - 547s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.3914 - val_acc: 0.7654\n",
      "Epoch 18/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 19/50\n",
      "870/870 [==============================] - 546s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 20/50\n",
      "870/870 [==============================] - 546s - loss: -3.4500 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7846\n",
      "Epoch 21/50\n",
      "870/870 [==============================] - 545s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7846\n",
      "Epoch 22/50\n",
      "870/870 [==============================] - 545s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.8206 - val_acc: 0.7462\n",
      "Epoch 23/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.3914 - val_acc: 0.7423\n",
      "Epoch 24/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5140 - val_acc: 0.7885\n",
      "Epoch 25/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2687 - val_acc: 0.7885\n",
      "Epoch 26/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2687 - val_acc: 0.7731\n",
      "Epoch 27/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7769\n",
      "Epoch 28/50\n",
      "870/870 [==============================] - 542s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 29/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.0848 - val_acc: 0.7769\n",
      "Epoch 30/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5753 - val_acc: 0.7692\n",
      "Epoch 31/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.9432 - val_acc: 0.7692\n",
      "Epoch 32/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.3300 - val_acc: 0.7615\n",
      "Epoch 33/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.6366 - val_acc: 0.7654\n",
      "Epoch 34/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2687 - val_acc: 0.7654\n",
      "Epoch 35/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.3300 - val_acc: 0.7692\n",
      "Epoch 36/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 37/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 38/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2687 - val_acc: 0.7808\n",
      "Epoch 39/50\n",
      "870/870 [==============================] - 542s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5753 - val_acc: 0.7462\n",
      "Epoch 40/50\n",
      "870/870 [==============================] - 540s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5753 - val_acc: 0.7923\n",
      "Epoch 41/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.5753 - val_acc: 0.7769\n",
      "Epoch 42/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.6366 - val_acc: 0.7654\n",
      "Epoch 43/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.6366 - val_acc: 0.7654\n",
      "Epoch 44/50\n",
      "870/870 [==============================] - 544s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.6366 - val_acc: 0.7577\n",
      "Epoch 45/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 46/50\n",
      "870/870 [==============================] - 542s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.4527 - val_acc: 0.7692\n",
      "Epoch 47/50\n",
      "870/870 [==============================] - 542s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2074 - val_acc: 0.7846\n",
      "Epoch 48/50\n",
      "870/870 [==============================] - 542s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.2074 - val_acc: 0.7769\n",
      "Epoch 49/50\n",
      "870/870 [==============================] - 543s - loss: -3.4512 - acc: 0.5402 - val_loss: -2.0848 - val_acc: 0.7923\n",
      "Epoch 50/50\n",
      "870/870 [==============================] - 542s - loss: -3.4512 - acc: 0.5402 - val_loss: -1.9621 - val_acc: 0.7923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x199d22fd0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 770 images belonging to 2 classes.\n",
    "Found 250 images belonging to 2 classes.\n",
    "Epoch 1/50\n",
    "770/770 [==============================] - 488s - loss: 0.6384 - acc: 0.7792 - val_loss: 0.7686 - val_acc: 0.6280\n",
    "Epoch 2/50\n",
    "770/770 [==============================] - 520s - loss: 0.3861 - acc: 0.8195 - val_loss: 0.5246 - val_acc: 0.7640\n",
    "Epoch 3/50\n",
    "770/770 [==============================] - 504s - loss: 0.3392 - acc: 0.8351 - val_loss: 0.5448 - val_acc: 0.7400\n",
    "Epoch 4/50\n",
    "770/770 [==============================] - 491s - loss: 0.3281 - acc: 0.8455 - val_loss: 0.5721 - val_acc: 0.6840\n",
    "Epoch 5/50\n",
    "770/770 [==============================] - 490s - loss: 0.3096 - acc: 0.8714 - val_loss: 0.4415 - val_acc: 0.8000\n",
    "Epoch 6/50\n",
    "770/770 [==============================] - 492s - loss: 0.2937 - acc: 0.8714 - val_loss: 0.4726 - val_acc: 0.7640\n",
    "Epoch 7/50\n",
    "770/770 [==============================] - 489s - loss: 0.2862 - acc: 0.8636 - val_loss: 0.4884 - val_acc: 0.7440\n",
    "Epoch 8/50\n",
    "770/770 [==============================] - 488s - loss: 0.2677 - acc: 0.8740 - val_loss: 0.5361 - val_acc: 0.7520\n",
    "Epoch 9/50\n",
    "770/770 [==============================] - 487s - loss: 0.2460 - acc: 0.8974 - val_loss: 0.4113 - val_acc: 0.8280\n",
    "Epoch 10/50\n",
    "770/770 [==============================] - 488s - loss: 0.2427 - acc: 0.9000 - val_loss: 0.3701 - val_acc: 0.8280\n",
    "Epoch 11/50\n",
    "770/770 [==============================] - 487s - loss: 0.2360 - acc: 0.8948 - val_loss: 0.3862 - val_acc: 0.8280\n",
    "Epoch 12/50\n",
    "770/770 [==============================] - 487s - loss: 0.2339 - acc: 0.8974 - val_loss: 0.4228 - val_acc: 0.8160\n",
    "Epoch 13/50\n",
    "770/770 [==============================] - 487s - loss: 0.2223 - acc: 0.9078 - val_loss: 0.3836 - val_acc: 0.8280\n",
    "Epoch 14/50\n",
    "770/770 [==============================] - 486s - loss: 0.2348 - acc: 0.8987 - val_loss: 0.3973 - val_acc: 0.8200\n",
    "Epoch 15/50\n",
    "770/770 [==============================] - 488s - loss: 0.1981 - acc: 0.9143 - val_loss: 0.3746 - val_acc: 0.8120\n",
    "Epoch 16/50\n",
    "770/770 [==============================] - 486s - loss: 0.1978 - acc: 0.9195 - val_loss: 0.3471 - val_acc: 0.8400\n",
    "Epoch 17/50\n",
    "770/770 [==============================] - 487s - loss: 0.1853 - acc: 0.9260 - val_loss: 0.3412 - val_acc: 0.8880\n",
    "Epoch 18/50\n",
    "770/770 [==============================] - 488s - loss: 0.1689 - acc: 0.9182 - val_loss: 0.3098 - val_acc: 0.8640\n",
    "Epoch 19/50\n",
    "770/770 [==============================] - 488s - loss: 0.1796 - acc: 0.9247 - val_loss: 0.3428 - val_acc: 0.8240\n",
    "Epoch 20/50\n",
    "770/770 [==============================] - 486s - loss: 0.1912 - acc: 0.9208 - val_loss: 0.3643 - val_acc: 0.8800\n",
    "Epoch 21/50\n",
    "770/770 [==============================] - 487s - loss: 0.1537 - acc: 0.9506 - val_loss: 0.3131 - val_acc: 0.8880\n",
    "Epoch 22/50\n",
    "770/770 [==============================] - 487s - loss: 0.1526 - acc: 0.9390 - val_loss: 0.2786 - val_acc: 0.9040\n",
    "Epoch 23/50\n",
    "770/770 [==============================] - 487s - loss: 0.1848 - acc: 0.9195 - val_loss: 0.3315 - val_acc: 0.8760\n",
    "Epoch 24/50\n",
    "770/770 [==============================] - 486s - loss: 0.1545 - acc: 0.9351 - val_loss: 0.2830 - val_acc: 0.9080\n",
    "Epoch 25/50\n",
    "770/770 [==============================] - 486s - loss: 0.1436 - acc: 0.9481 - val_loss: 0.3036 - val_acc: 0.8880\n",
    "Epoch 26/50\n",
    "770/770 [==============================] - 486s - loss: 0.1324 - acc: 0.9584 - val_loss: 0.3680 - val_acc: 0.8520\n",
    "Epoch 27/50\n",
    "770/770 [==============================] - 486s - loss: 0.1173 - acc: 0.9623 - val_loss: 0.2787 - val_acc: 0.9200\n",
    "Epoch 28/50\n",
    "770/770 [==============================] - 486s - loss: 0.1208 - acc: 0.9610 - val_loss: 0.2671 - val_acc: 0.9320\n",
    "Epoch 29/50\n",
    "770/770 [==============================] - 485s - loss: 0.1120 - acc: 0.9584 - val_loss: 0.2915 - val_acc: 0.9240\n",
    "Epoch 30/50\n",
    "770/770 [==============================] - 485s - loss: 0.1162 - acc: 0.9597 - val_loss: 0.2824 - val_acc: 0.9000\n",
    "Epoch 31/50\n",
    "770/770 [==============================] - 484s - loss: 0.1206 - acc: 0.9494 - val_loss: 0.2306 - val_acc: 0.9120\n",
    "Epoch 32/50\n",
    "770/770 [==============================] - 486s - loss: 0.1068 - acc: 0.9623 - val_loss: 0.3281 - val_acc: 0.9160\n",
    "Epoch 33/50\n",
    "770/770 [==============================] - 485s - loss: 0.0829 - acc: 0.9701 - val_loss: 0.2849 - val_acc: 0.9240\n",
    "Epoch 34/50\n",
    "770/770 [==============================] - 486s - loss: 0.0952 - acc: 0.9636 - val_loss: 0.3164 - val_acc: 0.9120\n",
    "Epoch 35/50\n",
    "770/770 [==============================] - 484s - loss: 0.0900 - acc: 0.9701 - val_loss: 0.3076 - val_acc: 0.9280\n",
    "Epoch 36/50\n",
    "770/770 [==============================] - 486s - loss: 0.0861 - acc: 0.9688 - val_loss: 0.2894 - val_acc: 0.9240\n",
    "Epoch 37/50\n",
    "770/770 [==============================] - 485s - loss: 0.0749 - acc: 0.9740 - val_loss: 0.2794 - val_acc: 0.9240\n",
    "Epoch 38/50\n",
    "770/770 [==============================] - 486s - loss: 0.0767 - acc: 0.9766 - val_loss: 0.2994 - val_acc: 0.9120\n",
    "Epoch 39/50\n",
    "770/770 [==============================] - 485s - loss: 0.0664 - acc: 0.9818 - val_loss: 0.2928 - val_acc: 0.9200\n",
    "Epoch 40/50\n",
    "770/770 [==============================] - 489s - loss: 0.0735 - acc: 0.9779 - val_loss: 0.2415 - val_acc: 0.9320\n",
    "Epoch 41/50\n",
    "770/770 [==============================] - 485s - loss: 0.0644 - acc: 0.9779 - val_loss: 0.3124 - val_acc: 0.9200\n",
    "Epoch 42/50\n",
    "770/770 [==============================] - 497s - loss: 0.0694 - acc: 0.9753 - val_loss: 0.3409 - val_acc: 0.8960\n",
    "Epoch 43/50\n",
    "770/770 [==============================] - 499s - loss: 0.0555 - acc: 0.9870 - val_loss: 0.3099 - val_acc: 0.9200\n",
    "Epoch 44/50\n",
    "770/770 [==============================] - 492s - loss: 0.0614 - acc: 0.9818 - val_loss: 0.2941 - val_acc: 0.9320\n",
    "Epoch 45/50\n",
    "770/770 [==============================] - 492s - loss: 0.0635 - acc: 0.9805 - val_loss: 0.3076 - val_acc: 0.9200\n",
    "Epoch 46/50\n",
    "770/770 [==============================] - 492s - loss: 0.0687 - acc: 0.9766 - val_loss: 0.2123 - val_acc: 0.9480\n",
    "Epoch 47/50\n",
    "770/770 [==============================] - 493s - loss: 0.1262 - acc: 0.9519 - val_loss: 0.2916 - val_acc: 0.9120\n",
    "Epoch 48/50\n",
    "770/770 [==============================] - 491s - loss: 0.0551 - acc: 0.9870 - val_loss: 0.2838 - val_acc: 0.9120\n",
    "Epoch 49/50\n",
    "770/770 [==============================] - 493s - loss: 0.0528 - acc: 0.9857 - val_loss: 0.3344 - val_acc: 0.9160\n",
    "Epoch 50/50\n",
    "770/770 [==============================] - 491s - loss: 0.0573 - acc: 0.9831 - val_loss: 0.4105 - val_acc: 0.9120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize, imsave\n",
    "im = preprocess_image_batch(['data2/nike-1.jpg'],img_size=(256,256), crop_size=(256,256), color_mode=\"bgr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize, imsave\n",
    "im = preprocess_image_batch(['data2/nike-1.jpg','data2/nike-2.jpg','data2/nike-3.jpg','data2/nike-4.jpg','data2/nike-5.jpg','data2/nike-6.jpg'],img_size=(256,256), crop_size=(256,256), color_mode=\"bgr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict on Nike-1.jpg = 1\n",
    "# predict on Nike-5.jpg = 0.4547\n",
    "out = model.predict(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.          18.99503899  16.08585548 ...,   0.           0.           0.        ]\n",
      "  [  0.          17.99496078  24.83306885 ...,   0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.          15.51640034 ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]]\n",
      "\n",
      " ..., \n",
      " [[  0.           0.           0.         ...,  23.81184006  41.98667908\n",
      "     0.16865906]\n",
      "  [  0.          14.62650585   0.         ...,  39.77188492  40.81079865\n",
      "     0.21136287]\n",
      "  [ 29.72748566  36.02740097  33.15977859 ...,   0.           0.           0.        ]\n",
      "  ..., \n",
      "  [ 10.70682526  27.776968    33.35593414 ...,  11.98501587   0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   5.48701859   0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           0.          15.55048847 ...,  17.75936317   0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   7.54284716   0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]]\n",
      "\n",
      " [[  0.           1.70392334   1.85563028 ...,  16.87734413  10.86275387\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...,  12.62855053  16.58658028\n",
      "     0.        ]\n",
      "  [  0.           0.           0.         ...,   9.5596056    0.           0.        ]\n",
      "  ..., \n",
      "  [  0.           9.84891891   0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           0.         ...,   0.           0.           0.        ]\n",
      "  [  0.           0.           6.47977018 ...,   0.           0.           0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2/nike-1.jpg\n"
     ]
    }
   ],
   "source": [
    " print 'data2/nike-{0}.jpg'.format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
